{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPQxuijuFZCnWLRNBwbRImT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batu-el/R252_MechInt/blob/main/Notebook3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNxKeSHb_7gM",
        "outputId": "7d9cdea4-15ac-4c38-aae3-ae045b753bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "# Add the directory to sys.path\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/R252_Project/progress-measures-paper')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install wandb\n",
        "\n",
        "import numpy as np\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import einops\n",
        "import random\n",
        "from helpers import *\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "import torch\n",
        "\n",
        "USE_WANDB = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLflrqqUAYuS",
        "outputId": "b1e7611e-cd89-4a24-d0d1-e7557bcfde8e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.40.6)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_WANDB:\n",
        "  import wandb\n",
        "  wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "lKkns5anAaqY",
        "outputId": "6e6a6714-cc2a-4b3b-cd31-9d782c568372"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen = True)\n",
        "class Config():\n",
        "    lr: float = 1e-3 #@param\n",
        "    weight_decay: float = 1.0 #@param\n",
        "    p: int = 113 #@param\n",
        "    d_model: int = 128 #@param\n",
        "    fn_name: str = 'add2' #@param ['add', 'subtract', 'x2xyy2','rand', 'add2']\n",
        "    frac_train: float = 0.3 #@param\n",
        "    num_epochs: int = 50000 #@param\n",
        "    save_models: bool = False #@param\n",
        "    save_every: int = 100 #@param\n",
        "\n",
        "    # TODO for the first 1000 steps, save every 10 because 'interesting stuff happens at the start'\n",
        "    # TODO add a helper function to generate indices here\n",
        "\n",
        "    # Stop training when test loss is <stopping_thresh\n",
        "    stopping_thresh: int = -1 #@param\n",
        "    seed: int = 0 #@param\n",
        "\n",
        "    num_layers: int = 1\n",
        "    batch_style: str = 'full'\n",
        "    d_vocab: int = p+1\n",
        "    n_ctx: int = 4\n",
        "    d_mlp: int = 4*d_model\n",
        "    num_heads: int = 4\n",
        "\n",
        "    act_type: str = 'ReLU' #@param ['ReLU', 'GeLU']\n",
        "\n",
        "\n",
        "    device: t.device = t.device(\"cuda\")\n",
        "\n",
        "    # TODO ankify the privileged basis concept- a priori vs etc. ; consider writing up an explanation of privileged basis\n",
        "\n",
        "    use_ln: bool = False\n",
        "\n",
        "    take_metrics_every_n_epochs: int = 1000 #@param\n",
        "\n",
        "    @property\n",
        "    def d_head(self):\n",
        "        return self.d_model // self.num_heads\n",
        "\n",
        "    @property\n",
        "    def random_answers(self):\n",
        "        return np.random.randint(low=0, high=self.p, size=(self.p, self.p))\n",
        "\n",
        "    @property\n",
        "    def fns_dict(self):\n",
        "        return {\n",
        "            'add': lambda x,y:(x+y) % self.p,\n",
        "            'subtract': lambda x,y:(x-y) % self.p,\n",
        "            'x2xyy2': lambda x,y:(x**2+x*y+y**2) % self.p,\n",
        "            'rand': lambda x,y:self.random_answers[x][y],\n",
        "            'add2': lambda flag,x,y,p: (x+y) % self.p,\n",
        "            }\n",
        "\n",
        "    @property\n",
        "    def fn(self):\n",
        "        return self.fns_dict[self.fn_name]\n",
        "\n",
        "    def is_train_is_test(self, train):\n",
        "        '''Creates an array of Boolean indices according to whether each data point is in train or test.\n",
        "        Used to index into the big batch of all possible data'''\n",
        "        # TODO probably the wrong place for this\n",
        "        is_train = []\n",
        "        is_test = []\n",
        "        for x in range(self.p):\n",
        "            for y in range(self.p):\n",
        "                if (x, y, 113) in train:\n",
        "                    is_train.append(True)\n",
        "                    is_test.append(False)\n",
        "                else:\n",
        "                    is_train.append(False)\n",
        "                    is_test.append(True)\n",
        "        is_train = np.array(is_train)\n",
        "        is_test = np.array(is_test)\n",
        "        return (is_train, is_test)\n",
        "\n",
        "    def is_it_time_to_save(self, epoch):\n",
        "        return (epoch % self.save_every == 0)\n",
        "\n",
        "    def is_it_time_to_take_metrics(self, epoch):\n",
        "        return epoch % self.take_metrics_every_n_epochs == 0\n",
        "\n",
        "# TODO make this an assert inside the consturctor\n",
        "assert Config.d_model % Config.num_heads == 0"
      ],
      "metadata": {
        "id": "muIYz6nEBbgm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HookPoint(nn.Module):\n",
        "    '''A helper class to get access to intermediate activations (inspired by Garcon)\n",
        "    It's a dummy module that is the identity function by default\n",
        "    I can wrap any intermediate activation in a HookPoint and get a convenient way to add PyTorch hooks\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fwd_hooks = []\n",
        "        self.bwd_hooks = []\n",
        "\n",
        "    def give_name(self, name):\n",
        "        # Called by the model at initialisation\n",
        "        self.name = name\n",
        "\n",
        "    def add_hook(self, hook, dir='fwd'):\n",
        "        # Hook format is fn(activation, hook_name)\n",
        "        # Change it into PyTorch hook format (this includes input and output,\n",
        "        # which are the same for a HookPoint)\n",
        "        def full_hook(module, module_input, module_output):\n",
        "            return hook(module_output, name=self.name)\n",
        "        if dir=='fwd':\n",
        "            handle = self.register_forward_hook(full_hook)\n",
        "            self.fwd_hooks.append(handle)\n",
        "        elif dir=='bwd':\n",
        "            handle = self.register_backward_hook(full_hook)\n",
        "            self.bwd_hooks.append(handle)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def remove_hooks(self, dir='fwd'):\n",
        "        if (dir=='fwd') or (dir=='both'):\n",
        "            for hook in self.fwd_hooks:\n",
        "                hook.remove()\n",
        "            self.fwd_hooks = []\n",
        "        if (dir=='bwd') or (dir=='both'):\n",
        "            for hook in self.bwd_hooks:\n",
        "                hook.remove()\n",
        "            self.bwd_hooks = []\n",
        "        if dir not in ['fwd', 'bwd', 'both']:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "xbPppUn0Bk0h"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "    '''Define network architecture\n",
        "    I defined my own transformer from scratch so I'd fully understand each component\n",
        "    - I expect this wasn't necessary or particularly important, and a bunch of this replicates existing Pyt functionality\n",
        "    '''\n",
        "    def __init__(self, d_vocab, d_model):\n",
        "        super().__init__()\n",
        "        self.W_E = nn.Parameter(t.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return t.einsum('dbp -> bpd', self.W_E[:, x])\n",
        "\n",
        "#| export\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, d_vocab, d_model):\n",
        "        super().__init__()\n",
        "        self.W_U = nn.Parameter(t.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x @ self.W_U)\n",
        "\n",
        "#| export\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, max_ctx, d_model):\n",
        "        super().__init__()\n",
        "        self.W_pos = nn.Parameter(t.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x+self.W_pos[:x.shape[-2]]\n",
        "\n",
        "#| export\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.w_ln = nn.Parameter(t.ones(d_model))\n",
        "        self.b_ln = nn.Parameter(t.zeros(d_model))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.model[0].use_ln:\n",
        "            x = x - x.mean(axis=-1)[..., None]\n",
        "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
        "            x = x * self.w_ln\n",
        "            x = x + self.b_ln\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "#| export\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.W_K = nn.Parameter(t.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
        "        self.W_Q = nn.Parameter(t.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
        "        self.W_V = nn.Parameter(t.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
        "        self.W_O = nn.Parameter(t.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n",
        "        self.register_buffer('mask', t.tril(t.ones((n_ctx, n_ctx))))\n",
        "        self.d_head = d_head\n",
        "        self.hook_k = HookPoint()\n",
        "        self.hook_q = HookPoint()\n",
        "        self.hook_v = HookPoint()\n",
        "        self.hook_z = HookPoint()\n",
        "        self.hook_attn = HookPoint()\n",
        "        self.hook_attn_pre = HookPoint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = self.hook_k(t.einsum('ihd,bpd->biph', self.W_K, x))\n",
        "        q = self.hook_q(t.einsum('ihd,bpd->biph', self.W_Q, x))\n",
        "        v = self.hook_v(t.einsum('ihd,bpd->biph', self.W_V, x))\n",
        "        attn_scores_pre = t.einsum('biph,biqh->biqp', k, q)\n",
        "        attn_scores_masked = t.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
        "        attn_matrix = self.hook_attn(F.softmax(self.hook_attn_pre(attn_scores_masked/np.sqrt(self.d_head)), dim=-1))\n",
        "        z = self.hook_z(t.einsum('biph,biqp->biqh', v, attn_matrix))\n",
        "        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n",
        "        out = t.einsum('df,bqf->bqd', self.W_O, z_flat)\n",
        "        return out\n",
        "\n",
        "#| export\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model, d_mlp, act_type, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.W_in = nn.Parameter(t.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
        "        self.b_in = nn.Parameter(t.zeros(d_mlp))\n",
        "        self.W_out = nn.Parameter(t.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
        "        self.b_out = nn.Parameter(t.zeros(d_model))\n",
        "        self.act_type = act_type\n",
        "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
        "        self.hook_pre = HookPoint()\n",
        "        self.hook_post = HookPoint()\n",
        "        assert act_type in ['ReLU', 'GeLU']\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hook_pre(t.einsum('md,bpd->bpm', self.W_in, x) + self.b_in)\n",
        "        if self.act_type=='ReLU':\n",
        "            x = F.relu(x)\n",
        "        elif self.act_type=='GeLU':\n",
        "            x = F.gelu(x)\n",
        "        x = self.hook_post(x)\n",
        "        x = t.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
        "        return x\n",
        "\n",
        "# export\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
        "        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
        "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
        "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
        "        self.hook_attn_out = HookPoint()\n",
        "        self.hook_mlp_out = HookPoint()\n",
        "        self.hook_resid_pre = HookPoint()\n",
        "        self.hook_resid_mid = HookPoint()\n",
        "        self.hook_resid_post = HookPoint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hook_resid_mid(x + self.hook_attn_out(self.attn((self.hook_resid_pre(x)))))\n",
        "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
        "        return x\n",
        "\n",
        "#| export\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config: Config, use_cache=False, use_ln=True):\n",
        "        '''this function could be augmented to contain more options for creating different architectures'''\n",
        "        super().__init__()\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        self.embed = Embed(d_vocab = config.d_vocab, d_model = config.d_model)\n",
        "        self.pos_embed = PosEmbed(max_ctx = config.n_ctx, d_model = config.d_model)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(d_model = config.d_model,\n",
        "            d_mlp = config.d_mlp,\n",
        "            d_head = config.d_head,\n",
        "            num_heads = config.num_heads,\n",
        "            n_ctx = config.n_ctx,\n",
        "            act_type = config.act_type,\n",
        "            model=[self]) for i in range(config.num_layers)])\n",
        "        self.unembed = Unembed(d_vocab = config.d_vocab, d_model = config.d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if type(module)==HookPoint:\n",
        "                module.give_name(name)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.pos_embed(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        # x = self.ln(x)\n",
        "        x = self.unembed(x)\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "    def hook_points(self):\n",
        "        return [module for name, module in self.named_modules() if 'hook' in name]\n",
        "\n",
        "    def remove_all_hooks(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks('fwd')\n",
        "            hp.remove_hooks('bwd')\n",
        "\n",
        "    def cache_all(self, cache, incl_bwd=False):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, name):\n",
        "            cache[name] = tensor.detach()\n",
        "        def save_hook_back(tensor, name):\n",
        "            cache[name+'_grad'] = tensor[0].detach()\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, 'fwd')\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, 'bwd')"
      ],
      "metadata": {
        "id": "nCvAo2YkBmVP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO move this into the config?\n",
        "import dataclasses\n",
        "from collections import defaultdict\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def gen_train_test(config: Config):\n",
        "    '''Generate train and test split'''\n",
        "    num_to_generate = config.p\n",
        "    flag = -1\n",
        "    pairs = [(flag, i, j, num_to_generate) for i in range(num_to_generate) for j in range(num_to_generate)]\n",
        "    # for pair in pairs:\n",
        "    #   print(pair)\n",
        "    #   print(config.fn(*pair))\n",
        "    #   break\n",
        "    labels = [config.fn(*pair) for pair in pairs]\n",
        "    pairs, labels = shuffle(pairs, labels, random_state=0)\n",
        "    div = int(config.frac_train*len(pairs))\n",
        "    train_X, train_y = pairs[:div], labels[:div]\n",
        "    test_X, test_y = pairs[div:], labels[div:]\n",
        "    return train_X, train_y, test_X, test_y\n",
        "\n",
        "def compute_accuracy(self, model, data):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# TODO what type for model?\n",
        "def full_loss(config : Config, model: Transformer, data_X, data_y):\n",
        "    '''Takes the cross entropy loss of the model on the data'''\n",
        "    # Take the final position only\n",
        "    logits = model(data_X)[:, -1]\n",
        "    labels = t.tensor(data_y).to(config.device)\n",
        "    return cross_entropy_high_precision(logits, labels)\n",
        "\n",
        "class Trainer:\n",
        "    '''TODO\n",
        "    ways this stinks:\n",
        "    - callbacks every k epochs\n",
        "    - training on infinite data\n",
        "    - general abstract class w/o assumption and subclasses w/ more assumptions\n",
        "    - check out hugging face trainer\n",
        "    - disentangle optimization step and taking gradients\n",
        "    - forward compatibility, e.g. batches per step\n",
        "    '''\n",
        "\n",
        "    def __init__(self, config : Config, log_to_wandb=False, model = None) -> None:\n",
        "        if log_to_wandb:\n",
        "          wandb.init(project = \"grokking\", config = dataclasses.asdict(config))\n",
        "        self.model = model if model is not None else Transformer(config, use_cache=False)\n",
        "        self.model.to(config.device)\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr = config.lr, weight_decay=config.weight_decay, betas=(0.9, 0.98))\n",
        "        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lambda step: min(step/10, 1)) # TODO make this a config option\n",
        "        self.run_name = f\"grok_{int(time.time())}\"\n",
        "        self.train_X,self.train_y, self.test_X, self.test_y = gen_train_test(config = config)\n",
        "        self.train, self.test = self.train_X, self.test_X\n",
        "        self.metrics_dictionary = defaultdict(dict) # so we can safely call 'update' on keys\n",
        "        print('training length = ', len(self.train))\n",
        "        print('testing length = ', len(self.test))\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.config = config\n",
        "        # added\n",
        "        self.train_accuracies = []\n",
        "        self.test_accuracies = []\n",
        "\n",
        "    def do_a_training_step(self, epoch: int):\n",
        "        '''returns train_loss, test_loss'''\n",
        "        # Compute Train Loss and Test Loss\n",
        "        train_loss = full_loss(config = self.config, model = self.model, data_X = self.train, data_y =self.train_y)\n",
        "        test_loss = full_loss(config = self.config, model = self.model, data_X = self.test, data_y = self.test_y)\n",
        "        self.train_losses.append(train_loss.item())\n",
        "        self.test_losses.append(test_loss.item())\n",
        "        # Compute Train Accuracy and Test Accuracy\n",
        "        # train_accuracy = self.compute_accuracy(self.model, self.train)\n",
        "        # test_accuracy = self.compute_accuracy(self.model, self.test)\n",
        "        # self.train_accuracies.append(train_accuracy)\n",
        "        # self.test_accuracies.append(test_accuracy)\n",
        "        # Report the Loss and\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}, train loss {t.log(train_loss).item():.4f}, test loss {t.log(test_loss).item():.4f}')\n",
        "            # print(f'Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "        train_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        return train_loss, test_loss"
      ],
      "metadata": {
        "id": "trJXfoD-Bn4f"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config =Config()\n",
        "log_to_wandb = False\n",
        "world = Trainer(config = config, log_to_wandb=log_to_wandb)\n",
        "print(f'Run name {world.run_name}')\n",
        "\n",
        "for epoch in range(config.num_epochs):\n",
        "    train_loss, test_loss = world.do_a_training_step(epoch)\n",
        "    if test_loss.item() < config.stopping_thresh:\n",
        "        break\n",
        "lines([world.train_losses, world.test_losses], labels=['train', 'test'], log_y=True)\n",
        "return world # to export the dictionary with the training metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeNI8mBXCDUV",
        "outputId": "435bac5d-2ab7-4629-c002-aaadd6af28c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training length =  3830\n",
            "testing length =  8939\n",
            "Run name grok_1709822135\n",
            "Epoch 0, train loss 1.5615, test loss 1.5611\n",
            "Epoch 100, train loss 0.9326, test loss 2.0140\n",
            "Epoch 200, train loss -3.8254, test loss 2.7997\n",
            "Epoch 300, train loss -4.8554, test loss 2.8177\n",
            "Epoch 400, train loss -5.9827, test loss 2.8670\n",
            "Epoch 500, train loss -7.0824, test loss 2.9196\n",
            "Epoch 600, train loss -8.1671, test loss 2.9721\n",
            "Epoch 700, train loss -9.2379, test loss 3.0233\n",
            "Epoch 800, train loss -10.2932, test loss 3.0711\n",
            "Epoch 900, train loss -11.3242, test loss 3.1152\n",
            "Epoch 1000, train loss -12.3049, test loss 3.1541\n",
            "Epoch 1100, train loss -13.2029, test loss 3.1864\n",
            "Epoch 1200, train loss -13.9393, test loss 3.2081\n",
            "Epoch 1300, train loss -14.4610, test loss 3.2175\n",
            "Epoch 1400, train loss -14.7176, test loss 3.2129\n",
            "Epoch 1500, train loss -14.8060, test loss 3.2010\n",
            "Epoch 1600, train loss -14.8406, test loss 3.1895\n",
            "Epoch 1700, train loss -14.8667, test loss 3.1818\n",
            "Epoch 1800, train loss -14.8786, test loss 3.1801\n",
            "Epoch 1900, train loss -14.8201, test loss 3.1833\n",
            "Epoch 2000, train loss -3.7020, test loss 3.1580\n",
            "Epoch 2100, train loss -5.6503, test loss 2.5277\n",
            "Epoch 2200, train loss -4.1602, test loss 2.2701\n",
            "Epoch 2300, train loss -4.8965, test loss 2.3515\n",
            "Epoch 2400, train loss -5.8495, test loss 2.4490\n",
            "Epoch 2500, train loss -6.8433, test loss 2.5398\n",
            "Epoch 2600, train loss -7.8546, test loss 2.6222\n",
            "Epoch 2700, train loss -8.8740, test loss 2.6970\n",
            "Epoch 2800, train loss -9.8933, test loss 2.7645\n",
            "Epoch 2900, train loss -10.9029, test loss 2.8254\n",
            "Epoch 3000, train loss -11.8857, test loss 2.8786\n",
            "Epoch 3100, train loss -12.8101, test loss 2.9218\n",
            "Epoch 3200, train loss -13.6303, test loss 2.9528\n",
            "Epoch 3300, train loss -14.2772, test loss 2.9684\n",
            "Epoch 3400, train loss -14.6942, test loss 2.9684\n",
            "Epoch 3500, train loss -14.8827, test loss 2.9575\n",
            "Epoch 3600, train loss -14.8238, test loss 2.9384\n",
            "Epoch 3700, train loss -14.7157, test loss 2.9176\n",
            "Epoch 3800, train loss -7.2274, test loss 2.5272\n",
            "Epoch 3900, train loss -4.8013, test loss 2.0926\n",
            "Epoch 4000, train loss -4.9996, test loss 2.0946\n",
            "Epoch 4100, train loss -5.9313, test loss 2.1827\n",
            "Epoch 4200, train loss -6.9196, test loss 2.2647\n",
            "Epoch 4300, train loss -7.9324, test loss 2.3353\n",
            "Epoch 4400, train loss -8.9565, test loss 2.3941\n",
            "Epoch 4500, train loss -9.9818, test loss 2.4403\n",
            "Epoch 4600, train loss -10.9993, test loss 2.4730\n",
            "Epoch 4700, train loss -11.9904, test loss 2.4910\n",
            "Epoch 4800, train loss -12.9238, test loss 2.4904\n",
            "Epoch 4900, train loss -13.7515, test loss 2.4658\n",
            "Epoch 5000, train loss -14.4094, test loss 2.4164\n",
            "Epoch 5100, train loss -14.8587, test loss 2.3456\n",
            "Epoch 5200, train loss -15.1009, test loss 2.2614\n",
            "Epoch 5300, train loss -5.7591, test loss 2.1910\n",
            "Epoch 5400, train loss -6.8759, test loss 1.5238\n",
            "Epoch 5500, train loss -6.4884, test loss 1.3399\n",
            "Epoch 5600, train loss -7.4616, test loss 1.2287\n",
            "Epoch 5700, train loss -8.5000, test loss 1.0535\n",
            "Epoch 5800, train loss -9.5645, test loss 0.7959\n",
            "Epoch 5900, train loss -10.6312, test loss 0.4476\n",
            "Epoch 6000, train loss -11.6790, test loss 0.0437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "world.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkWEXRAFCG6e",
        "outputId": "42e5feab-658c-41b5-cb05-e884a74cd207"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embed): Embed()\n",
              "  (pos_embed): PosEmbed()\n",
              "  (blocks): ModuleList(\n",
              "    (0): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (hook_k): HookPoint()\n",
              "        (hook_q): HookPoint()\n",
              "        (hook_v): HookPoint()\n",
              "        (hook_z): HookPoint()\n",
              "        (hook_attn): HookPoint()\n",
              "        (hook_attn_pre): HookPoint()\n",
              "      )\n",
              "      (mlp): MLP(\n",
              "        (hook_pre): HookPoint()\n",
              "        (hook_post): HookPoint()\n",
              "      )\n",
              "      (hook_attn_out): HookPoint()\n",
              "      (hook_mlp_out): HookPoint()\n",
              "      (hook_resid_pre): HookPoint()\n",
              "      (hook_resid_mid): HookPoint()\n",
              "      (hook_resid_post): HookPoint()\n",
              "    )\n",
              "  )\n",
              "  (unembed): Unembed()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfeayG0kHJH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}